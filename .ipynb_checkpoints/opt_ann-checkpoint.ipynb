{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d057a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install ray[tune]\n",
    "%matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from pareto import is_pareto_optimal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Capas de la red\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Transformaciones para normalizar los datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Cargar los datos de entrenamiento y prueba\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "def train_mnist(config):\n",
    "    net = Net()\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"])\n",
    "    train_loader = DataLoader(train_data, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "\n",
    "    n_epochs = 1\n",
    "    losses = np.zeros(n_epochs)\n",
    "    accuracies = np.zeros(n_epochs)\n",
    "    precisions = np.zeros((n_epochs, 10))\n",
    "    recalls = np.zeros((n_epochs, 10))\n",
    "    for e in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        n_samples = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = net(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n_samples += labels.size(0)\n",
    "            with torch.no_grad():\n",
    "                log_ps = net(images)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.sum(equals).item()\n",
    "                all_preds.extend(top_class.squeeze().tolist())\n",
    "                all_labels.extend(labels.tolist())\n",
    "        losses[e] = running_loss / len(train_loader)\n",
    "        accuracies[e] = accuracy / n_samples\n",
    "        precisions[e] = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "        recalls[e] = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "    return {\"loss\": losses[-1], \"accuracy\": accuracies[-1], \"precision\": precisions[-1], \"recall\": recalls[-1]}\n",
    "\n",
    "\n",
    "def test_best_model(best_config):\n",
    "    net = Net()\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=best_config[\"lr\"])\n",
    "    train_loader = DataLoader(train_data, batch_size=int(best_config[\"batch_size\"]), shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=int(best_config[\"batch_size\"]), shuffle=True)\n",
    "\n",
    "    for e in range(10):\n",
    "        running_loss = 0\n",
    "        running_precision = 0\n",
    "        running_recall = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = net(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate precision and recall for this batch\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "            precision = torch.sum(preds == labels).item() / len(labels)\n",
    "            recall = torch.sum((preds == labels) & (labels != 0)).item() / torch.sum(labels != 0).item()\n",
    "            running_precision += precision\n",
    "            running_recall += recall\n",
    "\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:  # <-- use test_loader instead of train_loader\n",
    "                log_ps = net(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "                # Calculate precision and recall for this batch\n",
    "                _, preds = torch.max(log_ps, dim=1)\n",
    "                precision += torch.sum(preds == labels).item() / len(labels)\n",
    "                recall += torch.sum((preds == labels) & (labels != 0)).item() / torch.sum(labels != 0).item()\n",
    "                # Append true and predicted labels for confusion matrix\n",
    "                y_true.extend(labels.tolist())\n",
    "                y_pred.extend(preds.tolist())\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        print(f\"Epoch {e+1}: Test loss: {test_loss/len(test_loader)}.. Test accuracy: {accuracy/len(test_loader)}.. Precision: {precision/len(test_loader)}.. Recall: {recall/len(test_loader)}..\")\n",
    "        print(f\"Confusion matrix:\\n{cm}\\n\")\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([32, 64, 128])\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=20,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "result = tune.run(\n",
    "    train_mnist,\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=20,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "test_best_model(best_trial.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract precision and recall values from the trials in result\n",
    "precision = []\n",
    "recall = []\n",
    "for trial in result.trials:\n",
    "    precision.append(trial.last_result[\"precision\"])\n",
    "    recall.append(trial.last_result[\"recall\"])\n",
    "precision = np.array(precision)\n",
    "recall = np.array(recall)\n",
    "\n",
    "# Concatenate precision and recall into a single (n_points, 2) array\n",
    "fitness = np.concatenate([precision.reshape(-1, 1), recall.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# Find the Pareto-optimal points\n",
    "is_efficient = is_pareto_optimal(fitness)\n",
    "\n",
    "# Filter the non-Pareto-optimal points\n",
    "pareto_front = fitness[is_efficient]\n",
    "\n",
    "# Plot the Pareto frontier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(precision, recall, alpha=0.5)\n",
    "ax.scatter(pareto_front[:, 0], pareto_front[:, 1], color=\"r\")\n",
    "ax.set_xlabel(\"Precision\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.set_title(\"Pareto Frontier\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
